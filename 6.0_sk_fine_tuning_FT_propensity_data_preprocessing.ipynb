{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to fine tune the XGBoost model and perform evaulation on user data for 7 day Free Trial.\n",
    "\n",
    "The whole process has been divided into 2 notebooks: \n",
    "\n",
    "- Part 1: Data Preprocessing: 6.0_sk_fine_tuning_FT_propensity_data_preprocessing.ipynb (this notebook)\n",
    "- part 2: Data Modeling and Evaluation: 6.0_sk_fine_tuning_FT_propensity_data_modeling.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE=101\n",
    "SMALL_DATASET=False\n",
    "\n",
    "BUCKET = \"datascience-hbo-users\"\n",
    "PREFIX = \"users/sk/FT_propensity/7_day\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Snowflake Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only when life gives you lemons (or you have issues connecting to snowflake)\n",
    "\n",
    "# %%bash\n",
    "# pip install --upgrade pip\n",
    "# pip3 install snowflake-connector-python\n",
    "# pip3 install sqlparse\n",
    "# # pip3 install botocore==1.14.17\n",
    "\n",
    "# # awscli-compatible botocore\n",
    "# pip3 install botocore==1.15.39\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (20.1.1)\n"
     ]
    }
   ],
   "source": [
    "# %%bash\n",
    "# # # # # # # # # # \n",
    "# # Libs config.\n",
    "# # # # # # # # # # \n",
    "\n",
    "# CFFI_VERSION=$(python -m pip list 2>/dev/null | grep cffi | awk '{print $2}')\n",
    "# # echo \"Current cffi version=\"$CFFI_VERSION\n",
    "\n",
    "# pip install --upgrade pip\n",
    "\n",
    "# if [[ \"$CFFI_VERSION\" == \"1.10.0\" ]]\n",
    "# then\n",
    "#     echo \"Uninstalling cffi\"\n",
    "#     pip uninstall --yes cffi\n",
    "# fi\n",
    "\n",
    "# yum_log=$(sudo yum history new && sudo yum install -y libffi-devel openssl-devel)\n",
    "# pip_log=$(python -m pip install --upgrade pip snowflake-connector-python tabulate ipywidgets sqlparse textile)\n",
    "# yum_log=$(jupyter nbextension enable --py widgetsnbextension 2> /dev/null)\n",
    "\n",
    "# if [[ \"$CFFI_VERSION\" == \"cffi (1.10.0)\" ]]\n",
    "# then \n",
    "#    echo \"Configuration has changed; restart notebook\"\n",
    "# fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "# import snowflake.connector\n",
    "# from abc import ABCMeta, abstractmethod\n",
    "\n",
    "# class Credentials(metaclass=ABCMeta):\n",
    "#     pass\n",
    "    \n",
    "    \n",
    "# class SSMPSCredentials(Credentials):\n",
    "#     def __init__(self, secretid: str):\n",
    "#         self._secretid = secretid\n",
    "#         self._secrets = {}\n",
    "        \n",
    "#     def get_keys(self):\n",
    "#         \"\"\"\n",
    "#         credential fetching \n",
    "#         \"\"\"\n",
    "#         _aws_sm_args = {'service_name': 'secretsmanager', 'region_name': 'us-east-1'}\n",
    "#         secrets_client = boto3.client(**_aws_sm_args)\n",
    "#         get_secret_value_response = secrets_client.get_secret_value(SecretId=self._secretid)\n",
    "#         return get_secret_value_response\n",
    "    \n",
    "    \n",
    "# class BaseConnector(metaclass=ABCMeta):\n",
    "#     @abstractmethod\n",
    "#     def connect(self):\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "\n",
    "# class SnowflakeConnector(BaseConnector):\n",
    "#     def __init__(self, credentials: Credentials):\n",
    "#         keys = credentials.get_keys()\n",
    "#         self._secrets = json.loads(keys.get('SecretString', \"{}\"))\n",
    "\n",
    "#     def connect(self, dbname: str, schema: str = 'DEFAULT'):\n",
    "#         ctx = snowflake.connector.connect(\n",
    "#             user=self._secrets['login_name'],\n",
    "#             password=self._secrets['login_password'],\n",
    "#             account=self._secrets['account'],\n",
    "#             warehouse=self._secrets['warehouse'],\n",
    "#             database=dbname,\n",
    "#             schema=schema\n",
    "#         )\n",
    "\n",
    "#         return ctx\n",
    "\n",
    "# MAX_FETCHED_RESULTSET_SIZE = MAX_QUERY_RETURN_SIZE\n",
    "\n",
    "# def execute_query(query: str):\n",
    "#     cur = ctx.cursor()\n",
    "#     try:\n",
    "#         cur.execute(query)\n",
    "#         while True:\n",
    "#             results = cur.fetchmany(MAX_FETCHED_RESULTSET_SIZE)\n",
    "#             if len(results) == 0:\n",
    "#                 break\n",
    "#             for row in results:\n",
    "#                 yield row\n",
    "#     finally:\n",
    "#         cur.close()\n",
    "         \n",
    "# def execute_insert_query(query: str):\n",
    "#     cur = ctx.cursor()\n",
    "#     try:\n",
    "#         cur.execute(query)\n",
    "#     finally:\n",
    "#         cur.close()\n",
    "\n",
    "\n",
    "# connector = SnowflakeConnector(SSMPSCredentials(SF_CREDS))\n",
    "# ctx = connector.connect(dbname=DB_NAME, schema=SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# //Following query is run on the Snowflake console. Dropping of columns have been moved from SageMaker to Snowflake intentionally to reduce the data preprocessing load from the Sagemaker environment\n",
    "# USE ROLE MAX_ANALYTICS;\n",
    "# USE WAREHOUSE MAX_ANALYTICS_USER;\n",
    "# USE DATABASE MAX_DEV;\n",
    "# USE SCHEMA WORKSPACE;\n",
    "\n",
    "# SHOW PARAMETERS LIKE 'WEEK_START%';\n",
    "# ALTER SESSION SET WEEK_START = 7;\n",
    "\n",
    "# /****************************\n",
    "# *   SET INPUT TABLE NAMES   *\n",
    "# *****************************/\n",
    "\n",
    "# SET ANALYSIS_START_DATE = '2020-05-27';\n",
    "# SET REFRESH_DATE = '2020-07-02';\n",
    "# SET FT_FULLJOIN_TABLE = 'MAX_DEV.WORKSPACE.MAX_FREETRIAL_BASE';\n",
    "# SET MODEL_INPUT_TABLE ='MAX_DEV.WORKSPACE.SK_FT_7_DAY_MODEL_INPUT_TABLE';\n",
    "\n",
    "# CREATE OR REPLACE TABLE IDENTIFIER($MODEL_INPUT_TABLE) AS\n",
    "# SELECT\n",
    "#     *\n",
    "# FROM IDENTIFIER($FT_FULLJOIN_TABLE)\n",
    "# WHERE (UPPER(PROVIDER) IN ('DTC', 'APPLE') AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-8,($REFRESH_DATE)))\n",
    "#     OR \n",
    "#     (UPPER(PROVIDER) IN ('SAMSUNG') AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-11,($REFRESH_DATE )))\n",
    "#     OR \n",
    "#     (UPPER(PROVIDER) IN ('GOOGLE') AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-13,($REFRESH_DATE )))\n",
    "# ;\n",
    "\n",
    "# //QA\n",
    "# SELECT\n",
    "#     'DESTINATION' AS CAT\n",
    "#     , PERIOD_RANK\n",
    "#     , COUNT(*) AS RECORDS\n",
    "#     , COUNT(DISTINCT HBO_UUID) AS COUNT_UUID\n",
    "# //    , MIN(FREE_TRIAL_START_DT)\n",
    "# //    , MAX(FREE_TRIAL_START_DT)\n",
    "# FROM  IDENTIFIER($MODEL_INPUT_TABLE)\n",
    "# GROUP BY PERIOD_RANK\n",
    "# UNION \n",
    "# SELECT\n",
    "#     'SOURCE' AS CAT\n",
    "#     , PERIOD_RANK\n",
    "#     , COUNT(*) AS RECORDS\n",
    "#     , COUNT(DISTINCT HBO_UUID) AS COUNT_UUID\n",
    "#     , MIN(FREE_TRIAL_START_DT)\n",
    "#     , MAX(FREE_TRIAL_START_DT)\n",
    "# FROM IDENTIFIER($FT_FULLJOIN_TABLE)\n",
    "# WHERE (UPPER(PROVIDER) IN ('DTC', 'APPLE')\n",
    "#        AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-8,($REFRESH_DATE)))\n",
    "#     OR \n",
    "#     (UPPER(PROVIDER) IN ('SAMSUNG')\n",
    "#      AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-11,($REFRESH_DATE )))\n",
    "#     OR \n",
    "#     (UPPER(PROVIDER) IN ('GOOGLE')\n",
    "#      AND FREE_TRIAL_START_DT BETWEEN ($ANALYSIS_START_DATE) AND DATEADD(DAY,-13,($REFRESH_DATE )))\n",
    "# GROUP BY PERIOD_RANK\n",
    "# ORDER BY PERIOD_RANK\n",
    "# ;\n",
    "\n",
    "\n",
    "# --##################################################################################################################\n",
    "# ----STEP 2: Drop unnecessary columns\n",
    "# --##################################################################################################################\n",
    "\n",
    "# ALTER TABLE IDENTIFIER($MODEL_INPUT_TABLE) \n",
    "# DROP COLUMN FREE_TRIAL_START_DT\n",
    "#             , FREE_TRIAL_END_DT\n",
    "#             , FIRST_STREAM_DATE_ALL_HIST\n",
    "#             , BILLING_CYCLE_START_DT\n",
    "#             , BILLING_CYCLE_END_DT\n",
    "#             , FIRST_WATCHED_ASSET_CLASS_SUB\n",
    "#             , TOTAL_HBONOW_WATCH_SEC\n",
    "#             , ANALYSIS_START_DT\n",
    "#             , ANALYSIS_END_DT           \n",
    "#             , FIRST_WATCHED_ASSET_TITLE_ADJ\n",
    "#             , TOTAL_WATCH_SEC\n",
    "#             , OUTLIER_DESC\n",
    "#             , NUM_DAYS_IN_FREE_TRIAL\n",
    "#             , FIRST_TURN_OFF_AUTORENEW_DATE\n",
    "#             , LAST_TURN_OFF_AUTORENEW_DATE\n",
    "#             , FIRST_STREAM_DATE_IN_WINDOW\n",
    "#             , FIRST_STREAM_DATE_IN_WINDOW_ADJ\n",
    "#             , LAST_STREAM_DATE_IN_WINDOW\n",
    "#             , LAST_STREAM_DATE_IN_WINDOW_ADJ\n",
    "#             , LAST_SESSION_START_TIME\n",
    "#             , FIRST_WATCHED_ASSET_TITLE\n",
    "#             , SERIES_VIEWING_CAT_LABEL\n",
    "# ;\n",
    "     \n",
    "# --##################################################################################################################\n",
    "# ----STEP 3: Copy data from Snowflake to S3\n",
    "# --##################################################################################################################\n",
    "\n",
    "# COPY INTO @DATASCIENCE_HBO_USERS/users/sk/FT_propensity/7_day/snowflake-hbomax-staging/raw_data.csv\n",
    "#      FROM IDENTIFIER($MODEL_INPUT_TABLE) \n",
    "#      FILE_FORMAT = (TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY = '\"' NULL_IF = ('missing') BINARY_FORMAT = utf8 COMPRESSION=NONE)\n",
    "#      OVERWRITE = TRUE\n",
    "#  --    SINGLE = TRUE\n",
    "#      HEADER = TRUE\n",
    "#      MAX_FILE_SIZE = 5368709120;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = f\"SELECT * FROM {TABLE_NAME} LIMIT 1000\"\n",
    "# query_columns = f\"SHOW COLUMNS IN {TABLE_NAME}\"\n",
    "\n",
    "# df_columns = list(pd.DataFrame(execute_query(query_columns)).iloc[:,2])\n",
    "# df_raw_sf = pd.DataFrame(execute_query(query),columns = df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1984"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for preprocessing data on remote instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocessing.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "print('modules import completed')\n",
    "\n",
    "def create_unique_index(data):\n",
    "#     idx_series = data.index.to_series()\n",
    "#     idx_gb = idx_series.groupby([data.HBO_UUID, data.PERIOD_RANK])\n",
    "#     idx_tf = idx_gb.transform(\"first\")\n",
    "#     data[\"UNIQUE_ID\"]=idx_tf\n",
    "    data[\"UNIQUE_ID\"]=data[\"HBO_UUID\"]+\":\"+data[\"PERIOD_RANK\"]\n",
    "    print(f\"data head: {data[['UNIQUE_ID', 'HBO_UUID', 'PERIOD_RANK']].head()}\")\n",
    "#     assert data.UNIQUE_ID.nunique()==data.HBO_UUID.nunique()*data.PERIOD_RANK.nunique()\n",
    "    df_step_1=data.set_index(['UNIQUE_ID'])\n",
    "    return df_step_1\n",
    "\n",
    "def fill_missing(data):\n",
    "    data.FIRST_WATCHED_ASSET_CLASS_SUB_ADJ = data.FIRST_WATCHED_ASSET_CLASS_SUB_ADJ.str.replace(\"unknown\",\"missing\")          \n",
    "    return data\n",
    "\n",
    "def transform(data):\n",
    "    print(f\"input data shape: {data.shape}\")\n",
    "    print(f\"input data size: {data.memory_usage(index=True).sum()}\")\n",
    "    print(f\"original dataframe head: {data.head()}\")\n",
    "    print(f\"original dataframe index name: {data.index.name}\")\n",
    "    print(f\"original dataframe index 5 values: {data.index[:5]}\")\n",
    "    cols_to_one_hot_encode=[\"PROVIDER\"\n",
    "                        , \"FIRST_WATCHED_ASSET_CLASS_SUB_ADJ\"\n",
    "                        , \"FT_SEGMENT\", \"FT_SUB_SEGMENT\"]\n",
    "\n",
    "    id_list=[\"UNIQUE_ID\", \"HBO_UUID\"]\n",
    "    cols_to_exclude_from_num_conversion=cols_to_one_hot_encode+id_list\n",
    "\n",
    "    obj_to_num_col_list=[col for col in data.select_dtypes(include=['object']).columns if col not in cols_to_exclude_from_num_conversion]\n",
    "\n",
    "    counter=0\n",
    "    for col in obj_to_num_col_list:\n",
    "        counter+=1\n",
    "        print(f\"will process column {col}\")\n",
    "        data[col]=data[col].astype(np.float16)\n",
    "        print(f\"processed column {col}\")\n",
    "        print(f\"No. of columns processed: {counter}\")\n",
    "     \n",
    "    print(\"size of data after converting object to numeric columns\")\n",
    "    print(f\"input data size: {data.memory_usage(index=True).sum()}\")\n",
    "            \n",
    "    encoder = OneHotEncoder(handle_unknown=\"ignore\", dtype=np.uint8)\n",
    "    print(f\"Columns to onehot encode: {cols_to_one_hot_encode}\")\n",
    "    encode_df=data[cols_to_one_hot_encode]\n",
    "    print(f\"shape of dataframe to be onehot encoded: {encode_df.shape}\")\n",
    "    encoded_mtx=encoder.fit_transform(encode_df).toarray()\n",
    "    encoded_df=pd.DataFrame(encoded_mtx, index=data.index, columns=encoder.get_feature_names())\n",
    "    print(\"size of categorical column data after one hot encoding\")\n",
    "    print(f\"encoded_df data size: {encoded_df.memory_usage(index=True).sum()}\")\n",
    "    print(f\"shape of categorical dataframe after onehot encoding: {encoded_df.shape}\")\n",
    "    print(f\"encoded dataframe head: {encoded_df.head()}\")\n",
    "    print(f\"encoded dataframe index name: {encoded_df.index.name}\")\n",
    "    print(f\"encoded dataframe index 5 values: {encoded_df.index[:5]}\")\n",
    "    \n",
    "    del encode_df\n",
    "    del encoded_mtx\n",
    "\n",
    "    df_step_1=pd.concat([data, encoded_df], axis=1)\n",
    "    \n",
    "    print(\"size of combined data set with onehot encoded columns\")    \n",
    "    print(f\"shape of full dataframe after merging onehot encoded dataframe: {df_step_1.shape}\")\n",
    "    print(f\"df_step_1 data size: {df_step_1.memory_usage(index=True).sum()}\")\n",
    "    \n",
    "    del data\n",
    "    del encoded_df\n",
    "    \n",
    "    # Keeping column \"FT_SEGMENT\" in the final to join it back while presenting results\n",
    "    cols_to_drop=[col for col in cols_to_one_hot_encode if col not in \"FT_SEGMENT\"]\n",
    "    # If we do not need columns in the final dataframe, we should drop cols_to_one_hot_encode\n",
    "    df_step_2=df_step_1.drop(cols_to_drop, axis=1)\n",
    "    del df_step_1\n",
    "    df_step_2[\"UNIQUE_ID\"]=df_step_2.index\n",
    "    return df_step_2\n",
    "\n",
    "# def add_weight_col(data, weight):\n",
    "#     stream_mask = (data[\"NUM_STREAMS_ADJ\"]>0)\n",
    "#     dormant_mask = (data[\"NUM_STREAMS_ADJ\"]==0)\n",
    "#     data.loc[stream_mask, \"WEIGHT\"] = 1-weight\n",
    "#     data.loc[dormant_mask, \"WEIGHT\"] = weight\n",
    "#     cols=list(data.columns)\n",
    "#     cols.insert(1, cols.pop(cols.index(\"WEIGHT\")))\n",
    "#     data = data.loc[:, cols]\n",
    "#     return data\n",
    "\n",
    "def split_dataframe(df, strat_column, target, test_size=0.1):\n",
    "    df_train,df_test = train_test_split (df, test_size = test_size, stratify=df[[strat_column, target]], random_state=101)\n",
    "    df_train,df_val = train_test_split (df_train, test_size = test_size, stratify=df_train[[strat_column, target]], random_state=101)\n",
    "    \n",
    "    print(f\"Distribution by PERIOD_RANK and FLG_TARGET\")\n",
    "    \n",
    "    print(f\"train: {df_train.groupby(by=[strat_column, target]).size().reset_index().rename(columns={0:'COUNT_UUID'})}\")\n",
    "    print(f\"test: {df_test.groupby(by=[strat_column, target]).size().reset_index().rename(columns={0:'COUNT_UUID'})}\")\n",
    "    print(f\"val: {df_val.groupby(by=[strat_column, target]).size().reset_index().rename(columns={0:'COUNT_UUID'})}\")\n",
    "    \n",
    "    df_train_X,df_train_y = df_train.drop([target],axis=1), df_train[[target]]\n",
    "    df_test_X,df_test_y = df_test.drop([target],axis=1), df_test[[target]]\n",
    "    df_val_X,df_val_y = df_val.drop([target],axis=1), df_val[[target]]\n",
    "    return df_train_X, df_train_y , df_test_X, df_test_y, df_val_X, df_val_y\n",
    "\n",
    "if __name__=='__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--train-test-split-ratio', type=float, default=0.1)\n",
    "    parser.add_argument('--csv-weight', type=float, default=0.7)\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    # how train-test-split-ratio will pick up value from args.train_test_split_ratio\n",
    "    split_ratio = args.train_test_split_ratio\n",
    "    csv_weight = args.csv_weight\n",
    "    \n",
    "    input_data_path = os.path.join('/opt/ml/processing', 'input')\n",
    "    \n",
    "    print('reading csv files from s3')\n",
    "    all_files = glob.glob(os.path.join(input_data_path, \"*.csv\"))\n",
    "    print(f\"all files: {all_files}\")\n",
    "    df = pd.concat((pd.read_csv(f, header=0, encoding=\"utf-8\") for f in all_files))\n",
    "    \n",
    "    print(len(df.columns),'\\n')\n",
    "    print(df.columns)\n",
    "    \n",
    "    print('read file completed ')\n",
    "    \n",
    "    print('transorming input data')\n",
    "    \n",
    "    clean_df_step_1=create_unique_index(df)\n",
    "    clean_df_step_2=fill_missing(clean_df_step_1)\n",
    "    clean_df_step_3=transform(clean_df_step_2)\n",
    "#     clean_df_step_4=add_weight_col(clean_df_step_3, csv_weight)\n",
    "    \n",
    "    print('transformation completed','\\n')\n",
    "     \n",
    "    df_train_X, df_train_y, df_test_X, df_test_y, df_val_X, df_val_y=split_dataframe(clean_df_step_3, \"PERIOD_RANK\", \"FLG_TARGET\", split_ratio)\n",
    "    \n",
    "    print(f\"train X shape: {df_train_X.shape}\")\n",
    "    print(f\"train y shape: {df_train_y.shape}\")\n",
    "    print(f\"test X shape: {df_test_X.shape}\")\n",
    "    print(f\"test y shape: {df_test_y.shape}\")\n",
    "    print(f\"val X shape: {df_val_X.shape}\")\n",
    "    print(f\"val y shape: {df_val_y.shape}\")\n",
    "    \n",
    "    print(f\"data split according to period rank\")\n",
    "    \n",
    "    print(f\"train data: {df_train_X.PERIOD_RANK.value_counts(dropna=False)}\")\n",
    "    print(f\"test data: {df_test_X.PERIOD_RANK.value_counts(dropna=False)}\")\n",
    "    print(f\"val data: {df_val_X.PERIOD_RANK.value_counts(dropna=False)}\")\n",
    "    \n",
    "    print(f\"Normalized data split according to period rank\")\n",
    "    \n",
    "    print(f\"Normalized train data: {df_train_X.PERIOD_RANK.value_counts(dropna=False, normalize=True)}\")\n",
    "    print(f\"Normalized test data: {df_test_X.PERIOD_RANK.value_counts(dropna=False, normalize=True)}\")\n",
    "    print(f\"Normalized val data: {df_val_X.PERIOD_RANK.value_counts(dropna=False, normalize=True)}\")\n",
    "    \n",
    "    print('Extracting side info')\n",
    "    \n",
    "    side_info_cols=[\"UNIQUE_ID\", \"HBO_UUID\", \"PERIOD_RANK\", \"FT_SEGMENT\"]\n",
    "    df_train_side_info=df_train_X[side_info_cols]\n",
    "    df_test_side_info=df_test_X[side_info_cols]\n",
    "    df_val_side_info=df_val_X[side_info_cols]\n",
    "    \n",
    "    # keep PERIOD_RANK in the train, test and val sets\n",
    "    cols_to_drop=[col for col in side_info_cols if col!=\"PERIOD_RANK\"]\n",
    "    df_train_X = df_train_X.drop(cols_to_drop, axis=1)\n",
    "    df_test_X = df_test_X.drop(cols_to_drop, axis=1)\n",
    "    df_val_X = df_val_X.drop(cols_to_drop, axis=1)\n",
    "    \n",
    "    assert \"PERIOD_RANK\" in df_train_X.columns\n",
    "    assert \"PERIOD_RANK\" in df_test_X.columns\n",
    "    assert \"PERIOD_RANK\" in df_val_X.columns\n",
    "    \n",
    "    print(f\"Side info train X shape: {df_train_side_info.shape}\")\n",
    "    print(f\"Side info test X shape: {df_test_side_info.shape}\")\n",
    "    print(f\"Side info val X shape: {df_val_side_info.shape}\")\n",
    "    \n",
    "    print(f\"train X shape: {df_train_X.shape}\")\n",
    "    print(f\"train y shape: {df_train_y.shape}\")\n",
    "    print(f\"test X shape: {df_test_X.shape}\")\n",
    "    print(f\"test y shape: {df_test_y.shape}\")\n",
    "    print(f\"val X shape: {df_val_X.shape}\")\n",
    "    print(f\"val y shape: {df_val_y.shape}\")\n",
    "    \n",
    "    print(\"Extracted side info\")\n",
    "    \n",
    "    print(\"Extracting column names\")\n",
    "    \n",
    "    print(f\"Number of train columns: {df_train_X.shape[1]}\")\n",
    "    print(f\"Number of test columns: {df_test_X.shape[1]}\")\n",
    "    print(f\"Number of val columns: {df_val_X.shape[1]}\")\n",
    "\n",
    "    df_train_X.columns.to_series().to_csv(\"/opt/ml/processing/train/train_columns.csv\", header = False, index = False)\n",
    "    df_test_X.columns.to_series().to_csv(\"/opt/ml/processing/test/test_columns.csv\", header = False, index = False)\n",
    "    df_val_X.columns.to_series().to_csv(\"/opt/ml/processing/val/val_columns.csv\", header = False, index = False)\n",
    "        \n",
    "    print(\"Extracted column names\")\n",
    "        \n",
    "    print(\"writing files back to s3\")\n",
    "    \n",
    "    # change headers back to False after validation\n",
    "    pd.concat([df_train_y,df_train_X],axis=1).to_csv(\"/opt/ml/processing/train/train.csv\", header = 0, index = False)\n",
    "    pd.concat([df_val_y,df_val_X],axis=1).to_csv(\"/opt/ml/processing/val/val.csv\", header = 0, index = False)\n",
    "    pd.concat([df_test_y,df_test_X],axis=1).to_csv(\"/opt/ml/processing/test/test.csv\", header = 0, index = False)\n",
    "    \n",
    "    df_train_side_info.to_csv(\"/opt/ml/processing/train/train_side_info.csv\", header = 0, index = False)\n",
    "    df_test_side_info.to_csv(\"/opt/ml/processing/val/test_side_info.csv\", header = 0, index = False)\n",
    "    df_val_side_info.to_csv(\"/opt/ml/processing/test/val_side_info.csv\", header = 0, index = False)\n",
    "    \n",
    "    print(\"processing write to s3\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "smclient = boto3.Session().client('sagemaker')\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n",
    "                                     role=role,\n",
    "                                     instance_type='ml.r5.24xlarge',\n",
    "                                     output_kms_key='alias/aws/s3',\n",
    "                                     instance_count=1,\n",
    "                                     base_job_name='FT-Propensity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'session' will be renamed to 'sagemaker_session' in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  FT-Propensity-2020-07-17-15-29-16-240\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/snowflake-hbomax-staging', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-613630599026/FT-Propensity-2020-07-17-15-29-16-240/input/code/preprocessing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/model_input_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'test_data', 'S3Output': {'S3Uri': 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/model_input_data', 'LocalPath': '/opt/ml/processing/test', 'S3UploadMode': 'EndOfJob'}}, {'OutputName': 'val_data', 'S3Output': {'S3Uri': 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/model_input_data', 'LocalPath': '/opt/ml/processing/val', 'S3UploadMode': 'EndOfJob'}}]\n",
      "................................\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/externals/joblib/externals/cloudpickle/cloudpickle.py:47: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\u001b[0m\n",
      "\u001b[34mmodules import completed\u001b[0m\n",
      "\u001b[34mreading csv files from s3\u001b[0m\n",
      "\u001b[34mall files: ['/opt/ml/processing/input/raw_data.csv_0_0_0.csv', '/opt/ml/processing/input/raw_data.csv_2_0_0.csv', '/opt/ml/processing/input/raw_data.csv_7_1_0.csv', '/opt/ml/processing/input/raw_data.csv_4_3_0.csv', '/opt/ml/processing/input/raw_data.csv_5_4_0.csv', '/opt/ml/processing/input/raw_data.csv_5_1_0.csv', '/opt/ml/processing/input/raw_data.csv_6_3_0.csv', '/opt/ml/processing/input/raw_data.csv_2_3_0.csv', '/opt/ml/processing/input/raw_data.csv_2_1_0.csv', '/opt/ml/processing/input/raw_data.csv_3_1_0.csv', '/opt/ml/processing/input/raw_data.csv_5_5_0.csv', '/opt/ml/processing/input/raw_data.csv_4_4_0.csv', '/opt/ml/processing/input/raw_data.csv_5_6_0.csv', '/opt/ml/processing/input/raw_data.csv_7_7_0.csv', '/opt/ml/processing/input/raw_data.csv_6_2_0.csv', '/opt/ml/processing/input/raw_data.csv_1_7_0.csv', '/opt/ml/processing/input/raw_data.csv_1_4_0.csv', '/opt/ml/processing/input/raw_data.csv_3_7_0.csv', '/opt/ml/processing/input/raw_data.csv_7_4_0.csv', '/opt/ml/processing/input/raw_data.csv_6_6_0.csv', '/opt/ml/processing/input/raw_data.csv_6_7_0.csv', '/opt/ml/processing/input/raw_data.csv_4_0_0.csv', '/opt/ml/processing/input/raw_data.csv_2_5_0.csv', '/opt/ml/processing/input/raw_data.csv_6_5_0.csv', '/opt/ml/processing/input/raw_data.csv_0_7_0.csv', '/opt/ml/processing/input/raw_data.csv_1_5_0.csv', '/opt/ml/processing/input/raw_data.csv_7_5_0.csv', '/opt/ml/processing/input/raw_data.csv_5_7_0.csv', '/opt/ml/processing/input/raw_data.csv_4_5_0.csv', '/opt/ml/processing/input/raw_data.csv_7_6_0.csv', '/opt/ml/processing/input/raw_data.csv_6_1_0.csv', '/opt/ml/processing/input/raw_data.csv_0_2_0.csv', '/opt/ml/processing/input/raw_data.csv_5_0_0.csv', '/opt/ml/processing/input/raw_data.csv_6_4_0.csv', '/opt/ml/processing/input/raw_data.csv_1_3_0.csv', '/opt/ml/processing/input/raw_data.csv_2_6_0.csv', '/opt/ml/processing/input/raw_data.csv_6_0_0.csv', '/opt/ml/processing/input/raw_data.csv_0_3_0.csv', '/opt/ml/processing/input/raw_data.csv_4_2_0.csv', '/opt/ml/processing/input/raw_data.csv_0_6_0.csv', '/opt/ml/processing/input/raw_data.csv_5_3_0.csv', '/opt/ml/processing/input/raw_data.csv_1_1_0.csv', '/opt/ml/processing/input/raw_data.csv_7_3_0.csv', '/opt/ml/processing/input/raw_data.csv_1_6_0.csv', '/opt/ml/processing/input/raw_data.csv_1_2_0.csv', '/opt/ml/processing/input/raw_data.csv_0_1_0.csv', '/opt/ml/processing/input/raw_data.csv_2_2_0.csv', '/opt/ml/processing/input/raw_data.csv_4_7_0.csv', '/opt/ml/processing/input/raw_data.csv_5_2_0.csv', '/opt/ml/processing/input/raw_data.csv_4_1_0.csv', '/opt/ml/processing/input/raw_data.csv_3_4_0.csv', '/opt/ml/processing/input/raw_data.csv_2_4_0.csv', '/opt/ml/processing/input/raw_data.csv_3_3_0.csv', '/opt/ml/processing/input/raw_data.csv_7_2_0.csv', '/opt/ml/processing/input/raw_data.csv_3_0_0.csv', '/opt/ml/processing/input/raw_data.csv_4_6_0.csv', '/opt/ml/processing/input/raw_data.csv_2_7_0.csv', '/opt/ml/processing/input/raw_data.csv_3_2_0.csv', '/opt/ml/processing/input/raw_data.csv_0_5_0.csv', '/opt/ml/processing/input/raw_data.csv_3_5_0.csv', '/opt/ml/processing/input/raw_data.csv_0_4_0.csv', '/opt/ml/processing/input/raw_data.csv_3_6_0.csv', '/opt/ml/processing/input/raw_data.csv_7_0_0.csv', '/opt/ml/processing/input/raw_data.csv_1_0_0.csv']\u001b[0m\n",
      "\u001b[34m215 \n",
      "\u001b[0m\n",
      "\u001b[34mIndex(['HBO_UUID', 'PROVIDER', 'FLG_TARGET', 'PERIOD_RANK', 'NUM_PROFILE',\n",
      "       'NUM_ADULT_PROFILE', 'NUM_KID_PROFILE', 'FLG_TURN_OFF_AUTORENEW',\n",
      "       'TOTAL_HBONOW_WATCH_SEC_ADJ', 'ROKU_PERCENT_ADJ_NOW',\n",
      "       ...\n",
      "       'HBO_ACQUIRED_CONTENT_STREAMING_TIME_PERC_ADJ',\n",
      "       'HBO_ACQUIRED_CONTENT_NUM_EPI_COMPLETED_80_PERC_ADJ',\n",
      "       'HBOMAX_ACQUIRED_CONTENT_STREAMING_TIME_PERC_ADJ',\n",
      "       'HBOMAX_ACQUIRED_CONTENT_NUM_EPI_COMPLETED_80_PERC_ADJ',\n",
      "       'FRIENDS_STREAMING_SEC_ADJ', 'FRIENDS_STREAMING_PERC_ADJ',\n",
      "       'BBT_STREAMING_SEC_ADJ', 'BBT_STREAMING_PERC_ADJ', 'FT_SEGMENT',\n",
      "       'FT_SUB_SEGMENT'],\n",
      "      dtype='object', length=215)\u001b[0m\n",
      "\u001b[34mread file completed \u001b[0m\n",
      "\u001b[34mtransorming input data\u001b[0m\n",
      "\u001b[34mdata head:                                            UNIQUE_ID  ... PERIOD_RANK\u001b[0m\n",
      "\u001b[34m0  ed260580ada8b2c465a03f3045b2b520ff4828e60de2ec...  ...           3\u001b[0m\n",
      "\u001b[34m1  aa5efcf38f08f88cc7caf097be193c33339d38dc08a332...  ...           2\u001b[0m\n",
      "\u001b[34m2  7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb8...  ...           2\u001b[0m\n",
      "\u001b[34m3  b68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b9...  ...           1\u001b[0m\n",
      "\u001b[34m4  d7dbd4731833a27edf799b0377a115e847748808c6bb17...  ...           3\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 3 columns]\u001b[0m\n",
      "\u001b[34minput data shape: (6844509, 215)\u001b[0m\n",
      "\u001b[34minput data size: 11827311552\u001b[0m\n",
      "\u001b[34moriginal dataframe head:                                                                                              HBO_UUID  ...             FT_SUB_SEGMENT\u001b[0m\n",
      "\u001b[34mUNIQUE_ID                                                                                              ...                           \u001b[0m\n",
      "\u001b[34med260580ada8b2c465a03f3045b2b520ff4828e60de2ece...  ed260580ada8b2c465a03f3045b2b520ff4828e60de2ec...  ...  4: A Few User Interaction\u001b[0m\n",
      "\u001b[34maa5efcf38f08f88cc7caf097be193c33339d38dc08a3328...  aa5efcf38f08f88cc7caf097be193c33339d38dc08a332...  ...        10: Movie Exclusive\u001b[0m\n",
      "\u001b[34m7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb86...  7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb8...  ...        10: Movie Exclusive\u001b[0m\n",
      "\u001b[34mb68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b93...  b68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b9...  ...        10: Movie Exclusive\u001b[0m\n",
      "\u001b[34md7dbd4731833a27edf799b0377a115e847748808c6bb17c...  d7dbd4731833a27edf799b0377a115e847748808c6bb17...  ...         06: Series Dabbler\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 215 columns]\u001b[0m\n",
      "\u001b[34moriginal dataframe index name: UNIQUE_ID\u001b[0m\n",
      "\u001b[34moriginal dataframe index 5 values: Index(['ed260580ada8b2c465a03f3045b2b520ff4828e60de2eced8324ea4165687cad:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'aa5efcf38f08f88cc7caf097be193c33339d38dc08a3328c449c78bc4cfa6c43:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       '7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb86f06b895085791c40d:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'b68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b93ce78949ab86d99424:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'd7dbd4731833a27edf799b0377a115e847748808c6bb17cbd56234903b1798f5:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64'],\n",
      "      dtype='object', name='UNIQUE_ID')\u001b[0m\n",
      "\u001b[34msize of data after converting object to numeric columns\u001b[0m\n",
      "\u001b[34minput data size: 11827311552\u001b[0m\n",
      "\u001b[34mColumns to onehot encode: ['PROVIDER', 'FIRST_WATCHED_ASSET_CLASS_SUB_ADJ', 'FT_SEGMENT', 'FT_SUB_SEGMENT']\u001b[0m\n",
      "\u001b[34mshape of dataframe to be onehot encoded: (6844509, 4)\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34msize of categorical column data after one hot encoding\u001b[0m\n",
      "\u001b[34mencoded_df data size: 355914468\u001b[0m\n",
      "\u001b[34mshape of categorical dataframe after onehot encoding: (6844509, 44)\u001b[0m\n",
      "\u001b[34mencoded dataframe head:                                                     x0_apple  ...  x3_missing\u001b[0m\n",
      "\u001b[34mUNIQUE_ID                                                     ...            \u001b[0m\n",
      "\u001b[34med260580ada8b2c465a03f3045b2b520ff4828e60de2ece...         0  ...           0\u001b[0m\n",
      "\u001b[34maa5efcf38f08f88cc7caf097be193c33339d38dc08a3328...         1  ...           0\u001b[0m\n",
      "\u001b[34m7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb86...         1  ...           0\u001b[0m\n",
      "\u001b[34mb68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b93...         0  ...           0\u001b[0m\n",
      "\u001b[34md7dbd4731833a27edf799b0377a115e847748808c6bb17c...         1  ...           0\n",
      "\u001b[0m\n",
      "\u001b[34m[5 rows x 44 columns]\u001b[0m\n",
      "\u001b[34mencoded dataframe index name: UNIQUE_ID\u001b[0m\n",
      "\u001b[34mencoded dataframe index 5 values: Index(['ed260580ada8b2c465a03f3045b2b520ff4828e60de2eced8324ea4165687cad:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'aa5efcf38f08f88cc7caf097be193c33339d38dc08a3328c449c78bc4cfa6c43:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       '7bb0fb5e26644ac4107126c3d3eb284a66a8d6eb732cb86f06b895085791c40d:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'b68e5a014015e799c4dd095a764c2c03da3bbe61b1b2b93ce78949ab86d99424:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64',\n",
      "       'd7dbd4731833a27edf799b0377a115e847748808c6bb17cbd56234903b1798f5:0         3\\n1         2\\n2         2\\n3         1\\n4         3\\n         ..\\n124269    1\\n124270    1\\n124271    2\\n124272    1\\n124273    2\\nName: PERIOD_RANK, Length: 6844509, dtype: int64'],\n",
      "      dtype='object', name='UNIQUE_ID')\u001b[0m\n",
      "\u001b[34msize of combined data set with onehot encoded columns\u001b[0m\n",
      "\u001b[34mshape of full dataframe after merging onehot encoded dataframe: (6844509, 259)\u001b[0m\n",
      "\u001b[34mdf_step_1 data size: 12128469948\u001b[0m\n",
      "\u001b[34mtransformation completed \n",
      "\u001b[0m\n",
      "\u001b[34mDistribution by PERIOD_RANK and FLG_TARGET\u001b[0m\n",
      "\u001b[34mtrain:     PERIOD_RANK  FLG_TARGET  COUNT_UUID\u001b[0m\n",
      "\u001b[34m0             1           0      342605\u001b[0m\n",
      "\u001b[34m1             1           1      449403\u001b[0m\n",
      "\u001b[34m2             2           0      342604\u001b[0m\n",
      "\u001b[34m3             2           1      449403\u001b[0m\n",
      "\u001b[34m4             3           0      342605\u001b[0m\n",
      "\u001b[34m5             3           1      449403\u001b[0m\n",
      "\u001b[34m6             4           0      342604\u001b[0m\n",
      "\u001b[34m7             4           1      449403\u001b[0m\n",
      "\u001b[34m8             5           0      342604\u001b[0m\n",
      "\u001b[34m9             5           1      449403\u001b[0m\n",
      "\u001b[34m10            6           0      342604\u001b[0m\n",
      "\u001b[34m11            6           1      449403\u001b[0m\n",
      "\u001b[34m12            7           0      342604\u001b[0m\n",
      "\u001b[34m13            7           1      449404\u001b[0m\n",
      "\u001b[34mtest:     PERIOD_RANK  FLG_TARGET  COUNT_UUID\u001b[0m\n",
      "\u001b[34m0             1           0       42296\u001b[0m\n",
      "\u001b[34m1             1           1       55482\u001b[0m\n",
      "\u001b[34m2             2           0       42297\u001b[0m\n",
      "\u001b[34m3             2           1       55482\u001b[0m\n",
      "\u001b[34m4             3           0       42296\u001b[0m\n",
      "\u001b[34m5             3           1       55482\u001b[0m\n",
      "\u001b[34m6             4           0       42297\u001b[0m\n",
      "\u001b[34m7             4           1       55482\u001b[0m\n",
      "\u001b[34m8             5           0       42297\u001b[0m\n",
      "\u001b[34m9             5           1       55482\u001b[0m\n",
      "\u001b[34m10            6           0       42297\u001b[0m\n",
      "\u001b[34m11            6           1       55482\u001b[0m\n",
      "\u001b[34m12            7           0       42297\u001b[0m\n",
      "\u001b[34m13            7           1       55482\u001b[0m\n",
      "\u001b[34mval:     PERIOD_RANK  FLG_TARGET  COUNT_UUID\u001b[0m\n",
      "\u001b[34m0             1           0       38067\u001b[0m\n",
      "\u001b[34m1             1           1       49934\u001b[0m\n",
      "\u001b[34m2             2           0       38067\u001b[0m\n",
      "\u001b[34m3             2           1       49934\u001b[0m\n",
      "\u001b[34m4             3           0       38067\u001b[0m\n",
      "\u001b[34m5             3           1       49934\u001b[0m\n",
      "\u001b[34m6             4           0       38067\u001b[0m\n",
      "\u001b[34m7             4           1       49934\u001b[0m\n",
      "\u001b[34m8             5           0       38067\u001b[0m\n",
      "\u001b[34m9             5           1       49934\u001b[0m\n",
      "\u001b[34m10            6           0       38067\u001b[0m\n",
      "\u001b[34m11            6           1       49934\u001b[0m\n",
      "\u001b[34m12            7           0       38067\u001b[0m\n",
      "\u001b[34m13            7           1       49933\u001b[0m\n",
      "\u001b[34mtrain X shape: (5544052, 256)\u001b[0m\n",
      "\u001b[34mtrain y shape: (5544052, 1)\u001b[0m\n",
      "\u001b[34mtest X shape: (684451, 256)\u001b[0m\n",
      "\u001b[34mtest y shape: (684451, 1)\u001b[0m\n",
      "\u001b[34mval X shape: (616006, 256)\u001b[0m\n",
      "\u001b[34mval y shape: (616006, 1)\u001b[0m\n",
      "\u001b[34mdata split according to period rank\u001b[0m\n",
      "\u001b[34mtrain data: 7    792008\u001b[0m\n",
      "\u001b[34m3    792008\u001b[0m\n",
      "\u001b[34m1    792008\u001b[0m\n",
      "\u001b[34m6    792007\u001b[0m\n",
      "\u001b[34m5    792007\u001b[0m\n",
      "\u001b[34m4    792007\u001b[0m\n",
      "\u001b[34m2    792007\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: int64\u001b[0m\n",
      "\u001b[34mtest data: 7    97779\u001b[0m\n",
      "\u001b[34m6    97779\u001b[0m\n",
      "\u001b[34m5    97779\u001b[0m\n",
      "\u001b[34m4    97779\u001b[0m\n",
      "\u001b[34m2    97779\u001b[0m\n",
      "\u001b[34m3    97778\u001b[0m\n",
      "\u001b[34m1    97778\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: int64\u001b[0m\n",
      "\u001b[34mval data: 6    88001\u001b[0m\n",
      "\u001b[34m5    88001\u001b[0m\n",
      "\u001b[34m4    88001\u001b[0m\n",
      "\u001b[34m3    88001\u001b[0m\n",
      "\u001b[34m2    88001\u001b[0m\n",
      "\u001b[34m1    88001\u001b[0m\n",
      "\u001b[34m7    88000\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: int64\u001b[0m\n",
      "\u001b[34mNormalized data split according to period rank\u001b[0m\n",
      "\u001b[34mNormalized train data: 7    0.142857\u001b[0m\n",
      "\u001b[34m3    0.142857\u001b[0m\n",
      "\u001b[34m1    0.142857\u001b[0m\n",
      "\u001b[34m6    0.142857\u001b[0m\n",
      "\u001b[34m5    0.142857\u001b[0m\n",
      "\u001b[34m4    0.142857\u001b[0m\n",
      "\u001b[34m2    0.142857\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: float64\u001b[0m\n",
      "\u001b[34mNormalized test data: 7    0.142858\u001b[0m\n",
      "\u001b[34m6    0.142858\u001b[0m\n",
      "\u001b[34m5    0.142858\u001b[0m\n",
      "\u001b[34m4    0.142858\u001b[0m\n",
      "\u001b[34m2    0.142858\u001b[0m\n",
      "\u001b[34m3    0.142856\u001b[0m\n",
      "\u001b[34m1    0.142856\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: float64\u001b[0m\n",
      "\u001b[34mNormalized val data: 6    0.142857\u001b[0m\n",
      "\u001b[34m5    0.142857\u001b[0m\n",
      "\u001b[34m4    0.142857\u001b[0m\n",
      "\u001b[34m3    0.142857\u001b[0m\n",
      "\u001b[34m2    0.142857\u001b[0m\n",
      "\u001b[34m1    0.142857\u001b[0m\n",
      "\u001b[34m7    0.142856\u001b[0m\n",
      "\u001b[34mName: PERIOD_RANK, dtype: float64\u001b[0m\n",
      "\u001b[34mExtracting side info\u001b[0m\n",
      "\u001b[34mSide info train X shape: (5544052, 4)\u001b[0m\n",
      "\u001b[34mSide info test X shape: (684451, 4)\u001b[0m\n",
      "\u001b[34mSide info val X shape: (616006, 4)\u001b[0m\n",
      "\u001b[34mtrain X shape: (5544052, 253)\u001b[0m\n",
      "\u001b[34mtrain y shape: (5544052, 1)\u001b[0m\n",
      "\u001b[34mtest X shape: (684451, 253)\u001b[0m\n",
      "\u001b[34mtest y shape: (684451, 1)\u001b[0m\n",
      "\u001b[34mval X shape: (616006, 253)\u001b[0m\n",
      "\u001b[34mval y shape: (616006, 1)\u001b[0m\n",
      "\u001b[34mExtracted side info\u001b[0m\n",
      "\u001b[34mExtracting column names\u001b[0m\n",
      "\u001b[34mNumber of train columns: 253\u001b[0m\n",
      "\u001b[34mNumber of test columns: 253\u001b[0m\n",
      "\u001b[34mNumber of val columns: 253\u001b[0m\n",
      "\u001b[34mExtracted column names\u001b[0m\n",
      "\u001b[34mwriting files back to s3\u001b[0m\n",
      "\u001b[34mprocessing write to s3\u001b[0m\n",
      "\n",
      "CPU times: user 3.46 s, sys: 233 ms, total: 3.69 s\n",
      "Wall time: 32min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "\n",
    "output_destination = 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/model_input_data'\n",
    "input_source = 's3://datascience-hbo-users/users/sk/FT_propensity/7_day/snowflake-hbomax-staging'\n",
    "\n",
    "sklearn_processor.run(code='preprocessing.py',\n",
    "                      inputs=[ProcessingInput(\n",
    "                        source=input_source,\n",
    "                        destination='/opt/ml/processing/input')],\n",
    "                      outputs=[\n",
    "                               ProcessingOutput(output_name='train_data',\n",
    "                                                source='/opt/ml/processing/train',\n",
    "                                                destination=output_destination),\n",
    "                               \n",
    "                               ProcessingOutput(output_name='test_data',\n",
    "                                                source='/opt/ml/processing/test',\n",
    "                                                destination=output_destination),\n",
    "                          \n",
    "                               ProcessingOutput(output_name='val_data',\n",
    "                                               source='/opt/ml/processing/val',\n",
    "                                               destination=output_destination)\n",
    "                               \n",
    "                               \n",
    "                              ],\n",
    "                      arguments=['--train-test-split-ratio', '0.1']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
